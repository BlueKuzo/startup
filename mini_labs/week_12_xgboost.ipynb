{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7FlusfzZ_8q"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/mini_labs/week_12_xgboost.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# BYU CS 473 — XGBoost\n",
        "\n",
        "In this assignment, you will learn the core ideas behind XGBoost and apply the method to a dataset of your choice.\n",
        "We’ll connect the math from the textbook to hands-on modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Goals\n",
        "- Explain the XGBoost objective function and its components.\n",
        "- Define and use key terms: regularizer, second-order Taylor expansion, leaf weights, gain, and split criterion.\n",
        "- Apply XGBoost to a dataset, tune hyperparameters, and evaluate results.\n",
        "- Understand how XGBoost improves upon traditional boosting methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvClpS40afIy"
      },
      "source": [
        "## Part 1 — Key Concepts from the Textbook  \n",
        "\n",
        "Read through the definitions below. For each one, write a **1–2 sentence explanation in your own words**.  \n",
        "\n",
        "### 1. Regularizer  \n",
        "Equation (18.47):  \n",
        "$\\Omega(f) = \\gamma J + \\frac{1}{2} \\lambda \\sum_{j=1}^J w_j^2$  \n",
        "\n",
        "**Question:** Why does XGBoost penalize both the **number of leaves** and the **magnitude of leaf weights**?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FSHTaM5bnL5"
      },
      "source": [
        "XGBoost penalizes having too many leaves because complex trees can overfit by carving the data into tiny, overly-specific regions. It also penalizes large leaf weights because extreme predictions usually indicate overfitting to noise rather than learning a stable pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7u8d1fPbo1Z"
      },
      "source": [
        "### 2. Second-order Taylor Expansion of the Loss  \n",
        "Equation (18.49):  \n",
        "$L_m(F_m) \\approx \\sum_{i=1}^N \\Big[ \\ell(y_i, f_{m-1}(x_i)) + g_{im} F_m(x_i) + \\tfrac{1}{2} h_{im} F_m(x_i)^2 \\Big] + \\Omega(F_m)$  \n",
        "\n",
        "**Question:** How does including the **Hessian term** (curvature) make boosting more accurate compared to using only gradients?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbcMbhoObxfG"
      },
      "source": [
        "The Hessian tells the model how quickly the loss is changing (its curvature), so using it lets the algorithm take smarter, more precise steps toward the minimum instead of relying only on slope information. This leads to better optimization and typically faster, more accurate convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwtO80EYbyzT"
      },
      "source": [
        "### 3. Optimal Leaf Weights  \n",
        "Equation (18.54):  \n",
        "$w_j^* = - \\frac{G_{jm}}{H_{jm} + \\lambda}$  \n",
        "\n",
        "**Question:** What does this formula mean about how leaf weights are chosen?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ8yhL2Vb1Em"
      },
      "source": [
        "The optimal weight of a leaf is found by taking the negative total gradient for that leaf and scaling it by the total curvature plus regularization, so leaves with strong, consistent gradients get larger weights. The λ term prevents huge updates when curvature is small, stabilizing the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4puP4SLlb5x6"
      },
      "source": [
        "### 4. Gain of a Split  \n",
        "Equation (18.56):  \n",
        "$\\text{gain} = \\tfrac{1}{2}\\Bigg( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\Bigg) - \\gamma$  \n",
        "\n",
        "**Question:** Why does XGBoost reject splits with **negative gain**?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UNat8V3cFQY"
      },
      "source": [
        "A negative gain means the split would increase error or complexity more than it improves fit, which makes the model worse overall. XGBoost only keeps splits that clearly reduce the loss after accounting for regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_SetVjcHmW"
      },
      "source": [
        "## Part 2 — Visualizing Boosting  \n",
        "\n",
        "### 2.1 Bagging vs Boosting (Recap)  \n",
        "Describe in words how **bagging** and **boosting** differ in how they:  \n",
        "- Use data sampling  \n",
        "- Build models sequentially or in parallel  \n",
        "- Reduce bias vs variance  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging trains each model on a different random sample of the training data. Boosting trains each new model on the full dataset, but reweights the data so that points that were previously mispredicted get more emphasis.\n",
        "\n",
        "\n",
        "Bagging trains all models in parallel. Boosting trains models sequentially, with each new model correcting the mistakes of the previous ones.\n",
        "\n",
        "\n",
        "Bagging mainly reduces variance, while boosting primarily reduces bias."
      ],
      "metadata": {
        "id": "mhSRpBlmG2hX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TB0dxSKcoC-"
      },
      "source": [
        "## Part 3 — Implementing XGBoost on 2 Datasets  \n",
        "\n",
        "### Step 1 — Look at the example dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihoCOlQ_dab8",
        "outputId": "3e12151f-b351-402b-e949-ae8699474f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ],
      "source": [
        "# Example: load a dataset\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = xgb.XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    eta=0.1,        # learning rate\n",
        "    max_depth=3,    # tree depth\n",
        "    n_estimators=100,\n",
        "    reg_lambda=1.0, # L2 regularization\n",
        "    reg_alpha=0.0   # L1 regularization\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnQZdb9NepPA"
      },
      "source": [
        "### Step 2 — Implement XGboost on a dataset of your choice  \n",
        "- Example locations to find a dataset:  \n",
        "  - A built-in dataset (e.g. `load_digits`)  \n",
        "  - A Kaggle dataset  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8appA0WneKSI",
        "outputId": "e7d0f02a-e80c-45ea-f989-9627abe942ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Digits Dataset ===\n",
            "Model 1 Params: {'max_depth': 3, 'eta': 0.1, 'n_estimators': 100, 'reg_lambda': 1.0, 'reg_alpha': 0.0} -> Accuracy: 0.9639\n",
            "Model 2 Params: {'max_depth': 4, 'eta': 0.05, 'n_estimators': 200, 'reg_lambda': 1.5, 'reg_alpha': 0.5} -> Accuracy: 0.9611\n",
            "Model 3 Params: {'max_depth': 2, 'eta': 0.2, 'n_estimators': 250, 'reg_lambda': 2.0, 'reg_alpha': 1.0} -> Accuracy: 0.9611\n",
            "Model 4 Params: {'max_depth': 6, 'eta': 0.3, 'n_estimators': 60, 'reg_lambda': 0.5, 'reg_alpha': 0.0} -> Accuracy: 0.9694\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "params_list_digits = [\n",
        "    {\"max_depth\": 3, \"eta\": 0.1, \"n_estimators\": 100, \"reg_lambda\": 1.0, \"reg_alpha\": 0.0},\n",
        "    {\"max_depth\": 4, \"eta\": 0.05, \"n_estimators\": 200, \"reg_lambda\": 1.5, \"reg_alpha\": 0.5},\n",
        "    {\"max_depth\": 2, \"eta\": 0.2, \"n_estimators\": 250, \"reg_lambda\": 2.0, \"reg_alpha\": 1.0},\n",
        "    {\"max_depth\": 6, \"eta\": 0.3, \"n_estimators\": 60, \"reg_lambda\": 0.5, \"reg_alpha\": 0.0},\n",
        "]\n",
        "\n",
        "print(\"\\n=== Digits Dataset ===\")\n",
        "for i, params in enumerate(params_list_digits, 1):\n",
        "    model = xgb.XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=10,\n",
        "        eval_metric=\"mlogloss\",\n",
        "        **params\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Model {i} Params: {params} -> Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDOihhCHdcAq"
      },
      "source": [
        "### Step 3 — Experiment with Hyperparameters on your dataset and the Cancer dataset\n",
        "- Change `max_depth`, `eta`, or `n_estimators`.  \n",
        "- Add regularization with `reg_lambda` and `reg_alpha`.  \n",
        "- **Question:** How do these changes affect performance?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5aD9xqkdgtR"
      },
      "source": [
        "Deeper trees improved performance slightly, but not dramatically, because the dataset already separates well with simple boundaries.\n",
        "\n",
        "A moderate-to-high learning rate performed best here, likely because digits is a low-noise dataset and the model doesn’t need very careful small steps.\n",
        "\n",
        "Using too many trees can overfit or add unnecessary complexity. The best model found a sweet spot with 60 trees.\n",
        "\n",
        "Strong regularization prevents overfitting, but in a dataset like digits—clean, well-separated images—too much regularization hurts accuracy because it restricts the model’s ability to fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJn4fMq_eTGx"
      },
      "source": [
        "## Part 4 — Reflection  \n",
        "\n",
        "Answer the following in complete sentences:  \n",
        "1. What role does the **regularizer** play in preventing overfitting?  \n",
        "2. How does using the **second-order Taylor expansion** help optimize the trees?  \n",
        "3. What surprised you most when experimenting with hyperparameters?  \n",
        "4. Why is XGBoost considered both a **statistical innovation** (Taylor expansion, regularization) and a **computer science innovation** (scalability, out-of-core learning)?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnaftVXVeUIN"
      },
      "source": [
        "1. The regularizer keeps the model from getting too complex by penalizing too many leaves or extreme leaf weights, which helps prevent overfitting.\n",
        "\n",
        "2. Using both the gradient and the curvature of the loss lets XGBoost make more accurate updates when building trees, so it learns faster and predicts better.\n",
        "\n",
        "3. I was surprised that deeper trees didn't improve performance more.\n",
        "\n",
        "4. It’s a statistical innovation because it uses advanced optimization and regularization to improve predictions, and a computer science innovation because it runs very efficiently, supports parallel computation, and can handle very large datasets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}