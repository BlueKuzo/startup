{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a9b49a7a",
      "metadata": {
        "id": "a9b49a7a"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef4cb40",
      "metadata": {
        "id": "5ef4cb40"
      },
      "source": [
        "# BYU CS 473 Lab Week 11"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54282b77",
      "metadata": {
        "id": "54282b77"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "In this lab you will be comparing different boosting techniques, specifically the AdaBoost and LogitBoost algorithms.\n",
        "\n",
        "AdaBoost is a powerful ensemble learning algorithm designed to improve the performance of weak classifiers by combining them into a strong classifier. In AdaBoost, a sequence of simple models is trained iteratively, with each model focusing more on the instances that previous models misclassified. The algorithm works by assigning equal weights to all training samples at the beginning. After each iteration, the weights of misclassified samples are increased, causing subsequent base classifiers to focus on the harder cases. This adaptation helps the ensemble address both bias and variance, often resulting in a more accurate and robust model than any single weak classifier. AdaBoost aggregates the predictions of all weak learners using a weighted voting scheme, where each learner's contribution depends on its accuracy.\n",
        "\n",
        "LogitBoost is another boosting algorithm for classification that builds an ensemble of weak learners to predict class probabilities accurately. Unlike AdaBoost, which focuses on minimizing exponential loss, LogitBoost is designed to minimize logistic loss, making it especially suitable for tasks where well-calibrated probability estimates are important. LogitBoost works by fitting each weak learner in a stage-wise fashion. At each boosting round, the algorithm calculates pseudo-residuals called \"working responses\" and corresponding weights based on the current model's probability estimates. The base learner then fits these responses using weighted least squares. Iteratively, this process refines the ensemble so it improves the accuracy of predicted probabilities. LogitBoost is helpful because it tends to be more robust to noisy data and outliers compared to AdaBoost, and it provides reliable probability outputs that can be used in decision-making or as input for other models. This makes LogitBoost a strong choice when you want both high accuracy and meaningful probability estimates in your classification models.\n",
        "\n",
        "See Sections 18.5.3 and 18.5.4 for more details on these algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065cd3fa",
      "metadata": {
        "id": "065cd3fa"
      },
      "source": [
        "---\n",
        "## Grading standards   \n",
        "\n",
        "Your notebook will be graded on the following:\n",
        "\n",
        "* 40% Correct implementation of the AdaBoost algorithm\n",
        "* 40% Correct implementation of the LogitBoost algorithm\n",
        "* 20% Analysis of the different methods\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc9132da",
      "metadata": {
        "id": "dc9132da"
      },
      "source": [
        "#### Imports and Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "488254e9",
      "metadata": {
        "id": "488254e9",
        "outputId": "9a101c43-e6b5-4ef7-a076-3e933666f528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = load_dataset(\"AiresPucrs/adult-census-income\", split=\"train\").to_pandas()\n",
        "\n",
        "data.replace('?', pd.NA, inplace=True)\n",
        "data = data.dropna()\n",
        "\n",
        "# Data cleaning\n",
        "categorials = data.columns[data.dtypes == object].values\n",
        "encoders = {col:LabelEncoder() for col in categorials}\n",
        "for col in categorials:\n",
        "    data[col] = encoders[col].fit_transform(data[col])\n",
        "\n",
        "target_col = data.columns[-1]\n",
        "\n",
        "# Use this data for the single classifier and LogitBoost\n",
        "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Change target labels to -1 and 1 instead of 0 and 1 to work with AdaBoost algorithm\n",
        "# Use this data for AdaBoost\n",
        "data[target_col] = data[target_col].replace({0: -1, 1: 1})\n",
        "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
        "X_train_adaboost, X_test_adaboost, y_train_adaboost, y_test_adaboost = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eef5368e",
      "metadata": {
        "id": "eef5368e"
      },
      "source": [
        "### Single Classifier\n",
        "\n",
        "Below is a single decision tree classifier that has been fit to the data. Run the cell to see how it does on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4b81368a",
      "metadata": {
        "id": "4b81368a",
        "outputId": "5c7c6a5c-1684-404e-e504-4b1605cc437c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Classifier Accuracy: 0.7514\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      1.00      0.86      4533\n",
            "           1       0.00      0.00      0.00      1500\n",
            "\n",
            "    accuracy                           0.75      6033\n",
            "   macro avg       0.38      0.50      0.43      6033\n",
            "weighted avg       0.56      0.75      0.64      6033\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Single Decision Tree Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Detailed classification report with zero_division parameter\n",
        "print(classification_report(y_test, y_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69aab08b",
      "metadata": {
        "id": "69aab08b"
      },
      "source": [
        "### Implement AdaBoost\n",
        "\n",
        "Following Algorithm 18.1 on page 616 of the textbook, you will implement the AdaBoost algorithm. For the base classifier, use sklearn.DecisionTreeClassifier with max_depth=1 as used above for the single classifier example. After you finish your AdaBoost implementation, get the predictions of your model on X_test_adaboost and print the accuracy and classification report as done above for the single classifier.\n",
        "\n",
        "![Adaboost Algorithm](https://raw.githubusercontent.com/wingated/cs473/main/labs/images/lab_week_11_adaboost.png)\n",
        "\n",
        "Hints:\n",
        "\n",
        "* AdaBoost relies on class labels being {+1, -1}. To handle this, use X_train_adaboost/y_train_adaboost which have been correctly labeled.\n",
        "\n",
        "* w should be shape (N,) with one value per datapoint.\n",
        "\n",
        "* When computing alpha, add a small epsilon term (1e-10) to the denominator for numerical stability.\n",
        "\n",
        "* For each iteration, you should save your classifier and your alpha to use for the final prediction (last line of the pseudocode).\n",
        "\n",
        "* The sgn[] in the last line means you should take the sign of the weighted sum inside the brackets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ebedb8ae",
      "metadata": {
        "id": "ebedb8ae",
        "outputId": "3ea485b4-d190-4730-fc90-21739fba441a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual AdaBoost Accuracy: 0.8419\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.87      0.93      0.90      4533\n",
            "           1       0.73      0.57      0.64      1500\n",
            "\n",
            "    accuracy                           0.84      6033\n",
            "   macro avg       0.80      0.75      0.77      6033\n",
            "weighted avg       0.83      0.84      0.84      6033\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "M = 50  # number of boosting rounds\n",
        "epsilon = 1e-10  # small term for numerical stability\n",
        "N = X_train_adaboost.shape[0]\n",
        "\n",
        "# Initialize weights\n",
        "w = np.ones(N) / N\n",
        "\n",
        "# Lists to store classifiers and their weights\n",
        "classifiers = []\n",
        "alphas = []\n",
        "\n",
        "# AdaBoost algorithm\n",
        "for m in range(M):\n",
        "    # Fit a weak learner with current weights\n",
        "    clf = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "    clf.fit(X_train_adaboost, y_train_adaboost, sample_weight=w)\n",
        "    y_pred_train = clf.predict(X_train_adaboost)\n",
        "\n",
        "    # Compute weighted error\n",
        "    incorrect = (y_pred_train != y_train_adaboost)\n",
        "    err_m = np.sum(w * incorrect) / np.sum(w)\n",
        "\n",
        "    # Compute alpha\n",
        "    alpha_m = 0.5 * np.log((1 - err_m + epsilon) / (err_m + epsilon))\n",
        "\n",
        "    # Update weights\n",
        "    w = w * np.exp(alpha_m * incorrect * 2)  # incorrect=1 if misclassified, 0 otherwise\n",
        "    w /= np.sum(w)  # normalize weights\n",
        "\n",
        "    # Save classifier and alpha\n",
        "    classifiers.append(clf)\n",
        "    alphas.append(alpha_m)\n",
        "\n",
        "# Final prediction function\n",
        "def adaboost_predict(X):\n",
        "    final_pred = np.zeros(X.shape[0])\n",
        "    for clf, alpha in zip(classifiers, alphas):\n",
        "        final_pred += alpha * clf.predict(X)\n",
        "    return np.sign(final_pred)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred_adaboost_manual = adaboost_predict(X_test_adaboost)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test_adaboost, y_pred_adaboost_manual)\n",
        "print(f\"Manual AdaBoost Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(classification_report(y_test_adaboost, y_pred_adaboost_manual, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef1d248",
      "metadata": {
        "id": "5ef1d248"
      },
      "source": [
        "### Implement LogitBoost\n",
        "\n",
        "Following Algorithm 18.2 on page 617 of the textbook, implement the LogitBoost algorithm. As your base learner, use sklearn.DecisionTreeRegressor with max_depth=1.\n",
        "\n",
        "![LogitBoost Algorithm](https://raw.githubusercontent.com/wingated/cs473/main/labs/images/lab_week_11_logitboost.png)\n",
        "\n",
        "Hints:\n",
        "\n",
        "* LogitBoost uses class labels of {1, 0}. X_train/y_train should be formatted for this.\n",
        "\n",
        "* w will again be shape (N,), as will pi.\n",
        "\n",
        "* Similar to alpha in AdaBoost, when computing z, add a small epsilon term (1e-10) to the denominator for numerical stability.\n",
        "\n",
        "* Calculating $F_{m}$ will be as simple as using the DecisionTreeRegressor's fit function (using X and z for your data, and w for the sample_weight).\n",
        "\n",
        "* $f(x)$ will initially be all zeros of shape (N,).\n",
        "\n",
        "* Again, save each of your regressors for the final prediction step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d3a53cbf",
      "metadata": {
        "id": "d3a53cbf",
        "outputId": "49e5c52c-d212-4fa8-b71e-f2996980abe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual LogitBoost Accuracy: 0.8546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.94      0.91      4533\n",
            "           1       0.76      0.61      0.68      1500\n",
            "\n",
            "    accuracy                           0.85      6033\n",
            "   macro avg       0.82      0.77      0.79      6033\n",
            "weighted avg       0.85      0.85      0.85      6033\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "M = 50  # number of boosting rounds\n",
        "epsilon = 1e-10\n",
        "N = X_train.shape[0]  # for {0,1} labels\n",
        "\n",
        "# Initialize F(x) and pi\n",
        "f = np.zeros(N)\n",
        "pi = np.full(N, 0.5)\n",
        "\n",
        "# Lists to store regressors\n",
        "regressors = []\n",
        "\n",
        "# LogitBoost algorithm\n",
        "for m in range(M):\n",
        "    # Compute working response z\n",
        "    z = (y_train - pi) / (pi * (1 - pi) + epsilon)\n",
        "\n",
        "    # Compute weights w\n",
        "    w = pi * (1 - pi)\n",
        "\n",
        "    # Fit base learner to weighted least squares\n",
        "    reg = DecisionTreeRegressor(max_depth=1)\n",
        "    reg.fit(X_train, z, sample_weight=w)\n",
        "\n",
        "    # Update F(x)\n",
        "    f += 0.5 * reg.predict(X_train)\n",
        "\n",
        "    # Update pi\n",
        "    pi = 1 / (1 + np.exp(-2 * f))\n",
        "\n",
        "    # Save the regressor\n",
        "    regressors.append(reg)\n",
        "\n",
        "# Final prediction function\n",
        "def logitboost_predict(X):\n",
        "    f_pred = np.zeros(X.shape[0])\n",
        "    for reg in regressors:\n",
        "        f_pred += 0.5 * reg.predict(X)\n",
        "    # Convert to binary predictions {0,1}\n",
        "    return (f_pred > 0).astype(int)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred_logitboost = logitboost_predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred_logitboost)\n",
        "print(f\"Manual LogitBoost Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(classification_report(y_test, y_pred_logitboost, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30aa6a9b",
      "metadata": {
        "id": "30aa6a9b"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "Compare the accuracy of each model (single decision tree classifier, AdaBoost, and LogitBoost) and describe what you learned about the effectiveness of boosting methods (2-3 sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7793fb36",
      "metadata": {
        "id": "7793fb36"
      },
      "source": [
        "The Single Decision Tree's accuracy was reported as the lowest at 75.14%. The AdaBoost's accuracy was reported as a close second at 84.19%. The LogitBoost's accuracy was reported as the best at 85.46%.\n",
        "\n",
        "\n",
        "\n",
        "| Algorithm     | Strengths                            | Weaknesses                                                     | Best Use                                                     |\n",
        "| ------------- | ------------------------------------ | -------------------------------------------------------------- | ------------------------------------------------------------ |\n",
        "| Decision Tree | Simple, interpretable, fast          | Overfits, weak accuracy, poor with imbalanced classes          | Baseline, feature understanding                              |\n",
        "| AdaBoost      | High accuracy, reduces bias          | Sensitive to outliers, may overfit, not probability-calibrated | Standard classification, clean datasets                      |\n",
        "| LogitBoost    | Probability outputs, robust to noise | Slower, more complex                                           | Classification with probability needs, imbalanced/noisy data |\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}