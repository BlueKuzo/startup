{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "Welcome to your first lab for CS 473, Advanced Machine Learning.\n",
        "\n",
        "In machine learning, models often predict *unnormalized log probabilities*. These must often be converted into regular probabilities.\n",
        "\n",
        "In this lab, you will explore the log-sum-exp function, which is described in the text (Sec. 2.5.4).  You will code up several variants of the function, and compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "# Part 1: Logsumexp\n",
        "---\n",
        "## Setup: The Iris Dataset\n",
        "We'll begin by downloading the Iris dataset. The iris dataset is a simple, but very famous, dataset introduced to the world by RA Fisher (the “father” of modern statistics”) in 1939. The dataset has five columns:\n",
        "* sepal length (cm)\n",
        "* sepal width (cm)\n",
        "* petal length (cm)\n",
        "* petal width (cm)\n",
        "* class\n",
        "\n",
        "In order to get logits to play with, we'll first train a multinomial logistic regression model (Sec. 2.5.3).  This model naturally outputs logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j1m2KIHShNdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35ed9bb-c67f-4d69-cdce-73c434baf580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "ds = datasets.load_dataset( \"scikit-learn/iris\" )\n",
        "\n",
        "df = pd.DataFrame( ds['train'] )\n",
        "\n",
        "X = np.array( df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']] )\n",
        "Y = np.array( LabelEncoder().fit_transform( df['Species'] ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l5hV6PS0CwT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c688b2cd-8b4d-4112-9d57-2ffa01d127bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 150)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression().fit(X,Y)\n",
        "\n",
        "W = model.coef_\n",
        "b = model.intercept_\n",
        "\n",
        "b = np.reshape( b, (3,1))\n",
        "\n",
        "logits = np.dot( W, X.T ) + b\n",
        "\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Esta7gBS1v"
      },
      "source": [
        "---\n",
        "## Exercise 1: convert logits to probabilities\n",
        "\n",
        "Since our model outputs logits, they must be converted. To do this, we'll use the softmax function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E3sgOg5oAvCv"
      },
      "outputs": [],
      "source": [
        "def softmax(logits):\n",
        "  # logits is a numpy matrix of d x N, where:\n",
        "  # d is the number of classes\n",
        "  # N is the number of samples\n",
        "\n",
        "  logits_copy = logits.copy()\n",
        "  d, N = logits_copy.shape\n",
        "\n",
        "  for sample in range(N):\n",
        "    sum_exp = 0\n",
        "\n",
        "    # Exponentiate the logits and sum them together for each row\n",
        "    for data_class in range(d):\n",
        "      logits_copy[data_class, sample] = np.exp( logits_copy[data_class, sample] )\n",
        "      sum_exp += logits_copy[data_class, sample]\n",
        "\n",
        "    # Divide each exponential by the row's sum\n",
        "    for data_class in range(d):\n",
        "      logits_copy[data_class, sample] = logits_copy[data_class, sample] / sum_exp\n",
        "\n",
        "  return logits_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P11uGxg9BKp7",
        "outputId": "92fb4568-52a4-4913-9255-0449cd637722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.81747643e-01 1.82523424e-02 1.43429404e-08]\n",
            "[5.54234723e-06 2.39030160e-02 9.76091442e-01]\n"
          ]
        }
      ],
      "source": [
        "# print out test cases\n",
        "probs = softmax(logits)\n",
        "print (probs[:,0])\n",
        "print (probs[:,120])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoXi9mq3BS1w"
      },
      "source": [
        "### test cases\n",
        "probs = softmax( logits )\n",
        "\n",
        "probs[:,0]\n",
        "#### array([9.81803910e-01, 1.81960759e-02, 1.43430317e-08])\n",
        "probs[:,120]\n",
        "#### array([5.49519371e-06, 2.38812718e-02, 9.76113233e-01])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q64wxOyBS1w"
      },
      "source": [
        "---\n",
        "## Exercise 2: convert logits to probabilities\n",
        "\n",
        "Now, code up the logsumexp function.  What test cases should you use for this function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zFzz8bqZBS1w"
      },
      "outputs": [],
      "source": [
        "def logsumexp(logits):\n",
        "  # logits is a numpy matrix of d x N, where:\n",
        "  # d is the number of classes\n",
        "  # N is the number of samples\n",
        "\n",
        "  d, N = logits.shape\n",
        "  result = np.zeros(d)\n",
        "\n",
        "  for data_class in range(d):\n",
        "    class_max = logits[data_class, 0]\n",
        "    exp_sum = 0\n",
        "\n",
        "    # Find the class max\n",
        "    for sample in range(1, N):\n",
        "      if logits[data_class, sample] > class_max:\n",
        "        class_max = logits[data_class, sample]\n",
        "\n",
        "    # Subtract class max, then exponentiate and sum\n",
        "    for sample in range(N):\n",
        "      exp_sum += np.exp(logits[data_class, sample] - class_max)\n",
        "\n",
        "    # Save log(sum) + class_max\n",
        "    result[data_class] = np.log(exp_sum) + class_max\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7sGtB6lNBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26001103-3477-450f-9982-651aee4cf807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11.15016899  7.69932095  9.8202921 ]\n"
          ]
        }
      ],
      "source": [
        "# test cases\n",
        "print (logsumexp(logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmQ3Gv7KBKp8"
      },
      "source": [
        "What should be printed??\n",
        "\n",
        "I'm not sure I understand the question, but each value returned by logsumexp(logits) is the weighted sum of every value for that class. A larger number means that the given class is more prevalent, but it's not a clear ratio like softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9B45o8BS1w"
      },
      "source": [
        "---\n",
        "## Exercise 3: explore underflow / overlow\n",
        "\n",
        "First, code up a function that compares two distributions. This can be anything you want; you may consider things like the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "REO87dlmBS1w"
      },
      "outputs": [],
      "source": [
        "def compare_probs( probs1, probs2 ):\n",
        "  # Normalize probs2 to sum to 1 (convert logsumexp \"scores\" into probabilities)\n",
        "  probs2_norm = np.exp(probs2) / np.sum(np.exp(probs2))\n",
        "\n",
        "  # Take mean of probs1 across flowers to get one distribution per class\n",
        "  probs1_mean = np.mean(probs1, axis=1)\n",
        "\n",
        "  # Compute Mean Squared Error\n",
        "  mse = np.mean((probs1_mean - probs2_norm)**2)\n",
        "\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3A85oOGUBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d3ac5d-2a94-4a47-f937-69d915f73a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10135992344225586\n"
          ]
        }
      ],
      "source": [
        "probs1 = softmax( logits )\n",
        "probs2 = logsumexp( logits )\n",
        "print(compare_probs( probs1, probs2 ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niNMrfHJBS1w"
      },
      "source": [
        "Now, see what happens if you add (or subtract) a constant from logits. How big must the constant be before things start going haywire?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rYmP1Jj7BS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0598c1fa-aa80-4087-9327-d68e9f9b5fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C = 0, probs[:,0] = [9.81747643e-01 1.82523424e-02 1.43429404e-08]\n",
            "C = 100, probs[:,0] = [9.81747643e-01 1.82523424e-02 1.43429404e-08]\n",
            "C = 500, probs[:,0] = [9.81747643e-01 1.82523424e-02 1.43429404e-08]\n",
            "C = 700, probs[:,0] = [9.81747643e-01 1.82523424e-02 1.43429404e-08]\n",
            "C = 1000 caused overflow!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25308393.py:14: RuntimeWarning: overflow encountered in exp\n",
            "  logits_copy[data_class, sample] = np.exp( logits_copy[data_class, sample] )\n",
            "/tmp/ipython-input-25308393.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  logits_copy[data_class, sample] = logits_copy[data_class, sample] / sum_exp\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "C_values = [0, 100, 500, 700, 1000]\n",
        "\n",
        "for C in C_values:\n",
        "  probs = softmax(logits + C)\n",
        "\n",
        "  # Detect overflow or NaNs\n",
        "  if np.any(np.isnan(probs)) or np.any(np.isinf(probs)):\n",
        "    print(f\"C = {C} caused overflow!\")\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    print(f\"C = {C}, probs[:,0] = {probs[:,0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaPkTeNBS1w"
      },
      "source": [
        "Now convert the logits to 16-bit precision, and re-run your experiments. Analyze the differences you see (2-3 sentences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6oxJEheOBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607ca1c9-04c9-4c91-fbeb-00cca4333ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C = 0, probs[:,0] = [0.9814  0.01822 0.     ]\n",
            "C = 100 caused overflow!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25308393.py:14: RuntimeWarning: overflow encountered in exp\n",
            "  logits_copy[data_class, sample] = np.exp( logits_copy[data_class, sample] )\n",
            "/tmp/ipython-input-25308393.py:19: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  logits_copy[data_class, sample] = logits_copy[data_class, sample] / sum_exp\n"
          ]
        }
      ],
      "source": [
        "logits16 = logits.astype( np.float16 )\n",
        "\n",
        "# your code here\n",
        "for C in C_values:\n",
        "  probs = softmax(logits16 + C)\n",
        "\n",
        "  # Detect overflow or NaNs\n",
        "  if np.any(np.isnan(probs)) or np.any(np.isinf(probs)):\n",
        "    print(f\"C = {C} caused overflow!\")\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    print(f\"C = {C}, probs[:,0] = {probs[:,0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U82T5tJNBKp9"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "Not surprisingly, the lower-bit structure overflowed sooner than the higher-bit structure, by a significant margin. I'm not really sure what the take away is supposed to be beyond that..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZs7xOKBS1w"
      },
      "source": [
        "---\n",
        "## Exercise 4: cleanly compute log probabilities\n",
        "\n",
        "Sometimes, we want to compute log probabilities (which are different from logits), but we want to do so \"cleanly\", ie, while avoiding overflow / underflow. First, mathematically figure out what the log of the softmax is (ie, take the log of eq. 2.99), and then combine it with insights from coding up the logsumexp function. Hint: at the end of the day, you will simply shift each column by a per-column constant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Laus2v79BS1x"
      },
      "outputs": [],
      "source": [
        "def log_logsumexp( logits ):\n",
        "  # logits is a numpy matrix of d x N, where:\n",
        "  # d is the number of classes\n",
        "  # N is the number of samples\n",
        "\n",
        "  LSE = logsumexp(logits)\n",
        "  log_softmax = logits.copy()\n",
        "  d, N = log_softmax.shape\n",
        "\n",
        "  for data_class in range(d):\n",
        "    for sample in range(N):\n",
        "      log_softmax[data_class, sample] = log_softmax[data_class, sample] - LSE[data_class]\n",
        "\n",
        "  return log_softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "62UvuyhYBS1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db06b933-4afb-43b9-c3cb-8fe169f2612c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -3.8079597   -4.34215254 -20.5196698 ]\n"
          ]
        }
      ],
      "source": [
        "print(log_logsumexp(logits)[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxX04mG2zXOe"
      },
      "source": [
        "---\n",
        "# Part 2: Probability Fundamentals\n",
        "\n",
        "For the following exercises, you are encouraged to work both by hand and by code, whatever makes the most sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xsg221sSR_4"
      },
      "source": [
        "## Exercise 1a: Joint Probability Distributions\n",
        "\n",
        "You are given the following two binary variables, X and Y, that can each take on the values 0 or 1. Assuming X and Y are independent, calculate the joint probability table (2x2 table for P(X, Y)). Display as a numpy array.\n",
        "\n",
        "P(X=0) = 0.6\n",
        "\n",
        "P(X=1) = 0.4\n",
        "\n",
        "P(Y=0) = 0.7\n",
        "\n",
        "P(Y=1) = 0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = [.6, .4]\n",
        "Y = [.7, .3]"
      ],
      "metadata": {
        "id": "TEbSZ2H4T5Ih"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V7dgYZfrSQ22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8606203-3a4d-4aa7-9cd0-0d2479b12873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.42 0.18]\n",
            " [0.28 0.12]]\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "joint_prob = np.array([[X[0]*Y[0], X[0]*Y[1]], [X[1]*Y[0], X[1]*Y[1]]])\n",
        "print(joint_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKat0t8w3qWY"
      },
      "source": [
        "Next, compute the following conditional probabilities:\n",
        "\n",
        "P(X=0|Y=0) = ?\n",
        "\n",
        "P(X=0|Y=1) = ?\n",
        "\n",
        "P(Y=0|X=0) = ?\n",
        "\n",
        "P(Y=0|X=1) = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uoFygm9S3yPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e6f8c7-a8c5-48cf-d47c-6b9953e0a54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(X=0|Y=0) = 0.6\n",
            "P(X=0|Y=1) = 0.6\n",
            "P(Y=0|X=0) = 0.7\n",
            "P(Y=0|X=1) = 0.7\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "def con_prob(event, given):\n",
        "    return round((event / given), 3)\n",
        "\n",
        "px = joint_prob.sum(axis=1)\n",
        "py = joint_prob.sum(axis=0)\n",
        "\n",
        "first_x0y0 = con_prob(joint_prob[0,0], py[0])\n",
        "first_x0y1 = con_prob(joint_prob[0,1], py[1])\n",
        "first_y0x0 = con_prob(joint_prob[0,0], px[0])\n",
        "first_y0x1 = con_prob(joint_prob[1,0], px[1])\n",
        "\n",
        "print(\"P(X=0|Y=0) =\", first_x0y0)\n",
        "print(\"P(X=0|Y=1) =\", first_x0y1)\n",
        "print(\"P(Y=0|X=0) =\", first_y0x0)\n",
        "print(\"P(Y=0|X=1) =\", first_y0x1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30-Nz1XpBKp_"
      },
      "source": [
        "Compare the result of these conditional probabilities to the original marginal probabilities given. What does this say about the relationship between variable dependence and using conditional probabilities? Write 1-2 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2aliHHOBKp_"
      },
      "source": [
        "P(X=0) = 0.6, regardless of Y. Likewise, P(Y=0) = 0.7, regardless of X. This means that the variables are independent of each other, which further means that knowing one provides no information about the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSZjb3q-BKp_"
      },
      "source": [
        "## Exercise 1b: Joint Probability Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UJIRJYfBKp_"
      },
      "source": [
        "Now consider this joint distribution:\n",
        "\n",
        "|  | $Y = 0$ | $Y = 1$|\n",
        "| :------- | :------: | -------: |\n",
        "| $X = 0$  | 0.45  | 0.10  |\n",
        "| $X = 1$  | 0.25  | 0.20  |\n",
        "\n",
        "First, compute the marginals from the joint table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO0YVIRgBKqA",
        "outputId": "74e3abfe-36bd-4d39-c3db-c0cc10929ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(X=0) = 0.55\n",
            "P(X=1) = 0.45\n",
            "P(Y=0) = 0.7\n",
            "P(Y=1) = 0.3\n"
          ]
        }
      ],
      "source": [
        "joint_prob = np.array([[0.45, 0.10], [0.25, 0.20]])\n",
        "\n",
        "# P(X=0), P(X=1) → sum across Y (axis=1)\n",
        "px = joint_prob.sum(axis=1)\n",
        "\n",
        "# P(Y=0), P(Y=1) → sum across X (axis=0)\n",
        "py = joint_prob.sum(axis=0)\n",
        "\n",
        "print(\"P(X=0) =\", round(px[0], 3))\n",
        "print(\"P(X=1) =\", round(px[1], 3))\n",
        "print(\"P(Y=0) =\", round(py[0], 3))\n",
        "print(\"P(Y=1) =\", round(py[1], 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXZBcHmdBKqA"
      },
      "source": [
        "Compute the same conditional probabilities as above:\n",
        "\n",
        "P(X=0|Y=0) = ?\n",
        "\n",
        "P(X=0|Y=1) = ?\n",
        "\n",
        "P(Y=0|X=0) = ?\n",
        "\n",
        "P(Y=0|X=1) = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE-pAL5JBKqA",
        "outputId": "f51deb8b-c025-47eb-c163-4791c0055536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(X=0|Y=0) = 0.643\n",
            "P(X=0|Y=1) = 0.333\n",
            "P(Y=0|X=0) = 0.818\n",
            "P(Y=0|X=1) = 0.556\n"
          ]
        }
      ],
      "source": [
        "second_x0y0 = con_prob(joint_prob[0,0], py[0])\n",
        "second_x0y1 = con_prob(joint_prob[0,1], py[1])\n",
        "second_y0x0 = con_prob(joint_prob[0,0], px[0])\n",
        "second_y0x1 = con_prob(joint_prob[1,0], px[1])\n",
        "\n",
        "print(\"P(X=0|Y=0) =\", second_x0y0)\n",
        "print(\"P(X=0|Y=1) =\", second_x0y1)\n",
        "print(\"P(Y=0|X=0) =\", second_y0x0)\n",
        "print(\"P(Y=0|X=1) =\", second_y0x1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Jzez3zBKqA"
      },
      "source": [
        "Check if the independence property $P(X, Y) = P(X)P(Y)$ holds for any cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1jT-JfLBKqA",
        "outputId": "4953c152-1d52-4694-837d-e498673707ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = 0 and Y = 0 are not independent\n",
            "X = 0 and Y = 1 are not independent\n",
            "Y = 0 and X = 0 are not independent\n",
            "Y = 0 and X = 1 are not independent\n"
          ]
        }
      ],
      "source": [
        "# Your answer here\n",
        "if second_x0y0 == px[0] * py[0]:\n",
        "  print(\"X = 0 and Y = 0 are independent\")\n",
        "else:\n",
        "  print(\"X = 0 and Y = 0 are not independent\")\n",
        "\n",
        "if second_x0y1 == px[0] * py[1]:\n",
        "  print(\"X = 0 and Y = 1 are independent\")\n",
        "else:\n",
        "  print(\"X = 0 and Y = 1 are not independent\")\n",
        "\n",
        "if second_y0x0 == py[0] * px[0]:\n",
        "  print(\"Y = 0 and X = 0 are independent\")\n",
        "else:\n",
        "  print(\"Y = 0 and X = 0 are not independent\")\n",
        "\n",
        "if second_y0x1 == py[0] * px[1]:\n",
        "  print(\"Y = 0 and X = 1 are independent\")\n",
        "else:\n",
        "  print(\"Y = 0 and X = 1 are not independent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i03RH796BKqW"
      },
      "source": [
        "Compare P(X=0|Y=0) to P(X=0|Y=1), and discuss what this says about the dependence between these variables (1-2 sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h1GH1GxBKqW"
      },
      "source": [
        "P(X=0|Y=0) != P(X=0|Y=1), meaning that the variables are not independent of each other, meaning that if you know something about one, you know something about the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoJaiN9QBKqW"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elOEoNBWTOUj"
      },
      "source": [
        "---\n",
        "## Exercise 2: Bayes Theorem\n",
        "\n",
        "After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive\n",
        "for a serious disease, and that the test is 99% accurate (i.e., the probability of testing positive given that you\n",
        "have the disease is 0.99, as is the probability of testing negative given that you don’t have the disease). The\n",
        "good news is that this is a rare disease, striking only one in 10,000 people. What are the chances that you\n",
        "actually have the disease? (Show your calculations as well as giving the final result.)\n",
        "\n",
        "*Hint: write out the variables you know, and think about what you'll need to calculate to find the final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fWWmJ6FbCyyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4c39f2-90b7-42a9-a1e1-307c4fccea30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98%\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = 0.99\n",
        "diseased = 1 / 10000\n",
        "\n",
        "diseased_given_positive = (test_accuracy * diseased) / ((test_accuracy * diseased) + ((1 - test_accuracy) * (1 - diseased)))\n",
        "print(str(round((diseased_given_positive * 100), 3)) + \"%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}