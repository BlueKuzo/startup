{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "KL divergence is one of the most commonly used concepts in machine learning. Here, we'll explore the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "---\n",
        "## Exercise #1: Symmetry    \n",
        "\n",
        "KL divergence is a *measure*, but not a *metric*. This means that while it satisfies some properties of things like a distance metric, it does not satisfy all of them.\n",
        "\n",
        "For example, KL divergence is NOT symmetric. First, implement a function that calculates the KL divergence between two discrete distributions. Then,cCraft an example to demonstrate that it is not symmetric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j1m2KIHShNdC",
        "outputId": "dbe4e6d6-b61e-44a2-d3bb-e6ab38667303",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL(a || b) = 0.3680642071684971\n",
            "KL(b || a) = 0.5108256237659907\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kl(a,b):\n",
        "    # a is a n-dimensional distribution\n",
        "    # b is a n-dimensional distribution\n",
        "    #\n",
        "    # return: KL(a||b)\n",
        "\n",
        "    a = np.array(a, dtype=float)\n",
        "    b = np.array(b, dtype=float)\n",
        "\n",
        "    # Normalize to ensure they are probability distributions\n",
        "    a = a / np.sum(a)\n",
        "    b = b / np.sum(b)\n",
        "\n",
        "    # Avoid division by zero / log of zero by masking terms where a[i] == 0\n",
        "    mask = (a > 0)\n",
        "    return np.sum(a[mask] * np.log(a[mask] / b[mask]))\n",
        "\n",
        "# find an example where kl(a,b) != kl(b,a)\n",
        "a = [0.9, 0.1]\n",
        "b = [0.5, 0.5]\n",
        "\n",
        "print(\"KL(a || b) =\", kl(a, b))\n",
        "print(\"KL(b || a) =\", kl(b, a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQrUMRBrxTk9"
      },
      "source": [
        "---\n",
        "## Exercise #2: Triangle inequality\n",
        "\n",
        "Another property that KL divergence does not satisfy is the triangle inequality, which states that\n",
        "\n",
        "kl(a,c) <= kl(a,b)+kl(b,c)\n",
        "\n",
        "Prove that KL divergence does not satisfy the triangle inequality by crafting a counter-example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-v53jQ3zxTk9",
        "outputId": "e3664768-0066-4175-845d-72542ed62e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL(a||b) = 0.3680642071684971\n",
            "KL(b||c) = 0.5108256237659907\n",
            "KL(a||c) = 1.7577796618689758\n",
            "\n",
            "KL(a||c) = 1.7577796618689758\n",
            "KL(a||b) + KL(b||c) = 0.8788898309344878\n",
            "Is kl(a,c) <= kl(a,b)+kl(b,c)?\tNo\n"
          ]
        }
      ],
      "source": [
        "a = [0.9, 0.1]\n",
        "b = [0.5, 0.5]\n",
        "c = [0.1, 0.9]\n",
        "\n",
        "print(\"KL(a||b) =\", kl(a, b))\n",
        "print(\"KL(b||c) =\", kl(b, c))\n",
        "print(\"KL(a||c) =\", kl(a, c))\n",
        "\n",
        "left = kl(a, c)\n",
        "right = kl(a, b) + kl(b, c)\n",
        "\n",
        "print(\"\\nKL(a||c) =\", left)\n",
        "print(\"KL(a||b) + KL(b||c) =\", right)\n",
        "print(\"Is kl(a,c) <= kl(a,b)+kl(b,c)?\", end=\"\\t\")\n",
        "\n",
        "if left <= right:\n",
        "    print(\"Yes\")\n",
        "\n",
        "else:\n",
        "    print(\"No\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZgQ5x9XxTk9"
      },
      "source": [
        "---\n",
        "## Exercise #3: Proofs\n",
        "\n",
        "Prove that:\n",
        "\n",
        "1) kl(a,a) = 0\n",
        "2) kl(a,b) >= 0\n",
        "\n",
        "Extra credit:\n",
        "\n",
        "3) kl(a,b) = 0 iff a==b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prove that kl(a,a) = 0\n",
        "\n",
        "    * KL(a,b) = ∑((ai)(ln(ai/bi)))\n",
        "    * x/x = 1\n",
        "    * ln(1) = 0\n",
        "    * (x)(0) = 0\n",
        "\n",
        "    Given the above, if a == b, ai = bi for all i, so (ai)(ln(ai/bi)) = (ai)(ln(ai/ai)) = (ai)(ln(1)) = (ai)(0) = 0 for all i.\n",
        "\n",
        "    ∑0 = 0, therefore kl(a,a) = 0\n",
        "\n",
        "2. Prove that kl(a,b) >= 0\n",
        "\n",
        "    * ai >= 0, bi >= 0, ∑ai = ∑bi = 1\n",
        "    * KL(a,b) = ∑((ai)(ln(ai/bi)))\n",
        "    * lim(x→0) ln(x) = 0, so ln(ai) = 0 when ai = 0\n",
        "    * lim(x→0) ln(1/x) = ∞, so ln(ai/bi) = ∞ when ai > 0 and bi = 0\n",
        "    * Apply Gibbs Inequality:\n",
        "        * ln(x) <= x, x - 1 for x > 0\n",
        "        * therefore ln(bi/ai) <= (bi/ai) - 1\n",
        "        * multiplied by -1, we get -ln(bi/ai) >= 1 - (bi/ai)\n",
        "        * Because -ln(bi/ai) = ln(ai/bi), we get ln(ai/bi) >= 1 - (bi/ai)\n",
        "        * Multiply by ai and sum to get ∑((ai)(ln(ai/bi))) >= ∑(ai - bi) = ∑(ai) - ∑(bi) = 1 - 1 = 0\n",
        "    \n",
        "    Given all this, ∑((ai)(ln(ai/bi))) >= 0, or kl(a,b) >= 0.\n",
        "\n",
        "3. Prove that kl(a,b) = 0 iff a==b\n",
        "\n",
        "    * Combining the above proofs, we see that kl(a,b) = 0 when a == b, and kl(a,b) >= 0\n",
        "    * When ai > 0, (ai)(ln(ai/bi)) = 0 iff ln(ai/bi) == 0, which happens iff ai/bi == 1, which happens iff ai = bi\n",
        "    * Because ∑ai = ∑bi = 1, if ai ever equals 0 while bi != 0, a must, at some point, make up the difference, meaning there exists some aj > bj. If aj > bj, (aj)(ln(aj/bj)) > (aj)ln(1) = (aj)(0).\n",
        "\n",
        "    In order for (ai)(ln(ai/bi)) = 0 when ai != 0, ai must be equal to bi. For (ai)(ln(ai/bi)) = 0 when ai = 0 and bi != 0, there must be another value of i where ai > bi, ln(ai/bi) > ln(1), ln(ai/bi) > 0. Therefore, for no value of ai can (ai)(ln(ai/bi)) = 0 unless ai = bi. Given that ai >= 0, bi >= 0, kl(a,b) = 0 iff (ai)(ln(ai/bi)) = 0 for all i. Putting all of this together, kl(a,b) = 0 iff a==b."
      ],
      "metadata": {
        "id": "L_qGxhO638HY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}