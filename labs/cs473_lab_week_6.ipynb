{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "The concept of mutual information is powerful in theory, but in practice it can be difficult to estimate. In this lab, you will implement a conceptually simple way to estimate the mutual information between two random variables. It is known as the KSG estimator (from \"Kraskov–Stögbauer–Grassberger\").\n",
        "\n",
        "You will then use your estimator as part of a feature selection algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "---\n",
        "## Grading standards   \n",
        "\n",
        "Your notebook will be graded on the following:\n",
        "\n",
        "* 60% Correct implementation of KSG estimator\n",
        "* 20% Correct implementation of \"all_mutual_inf\"\n",
        "* 20% Correct selection of most informative columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywWcDzoi9cEv"
      },
      "source": [
        "---\n",
        "## Description\n",
        "\n",
        "For this lab, we will use this dataset [lab_week_6_data.csv](https://github.com/wingated/cs473/blob/main/labs/data/lab_week_6_data.csv).\n",
        "\n",
        "Download the csv from the link above, then upload the csv to your current google colab runtime.\n",
        "\n",
        "Then begin by loading and preparing the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U-golbf-9cEv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv( \"lab_week_6_data.csv\" )\n",
        "\n",
        "X = df[['x1','x2','x3','x4','x5', 'x6', 'x7']]\n",
        "y = df['y']\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhj19dgo9cEw"
      },
      "source": [
        "The KSG estimator is an elegant idea that's fairly easy to implement. The idea is to use an adaptive grid, where the grid depends on the local density of the data.  This is accomplished by calculating the k'th nearest neighbor for each point, and then defining a box around each point that just barely extends to include the k'th nearest neighbor. Importantly, the size of the box is also used in the marginal entropy calculations\n",
        "\n",
        "To get some intuition for the method, check out [this demonstration on Wolfram Alpha's site](https://demonstrations.wolfram.com/KraskovKSGEstimatorOfMutualInformation/).  Here is a screenshot of their UI:\n",
        "\n",
        "![Wolfram UI](https://raw.githubusercontent.com/wingated/cs473/main/labs/images/lab_week_6_image1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrQZmJ4L9cEx"
      },
      "source": [
        "The algorithm is fairly straightforward.  We are given N samples from the joint distribution of p(X,Y). First, we calculate the *local mutual information* for each data point:\n",
        "\n",
        "1. For each datapoint $d_j = (x_j,y_j)$\n",
        "2. Find the k'th nearest neighbor (call this point $n_j$) -- you should use the infinity norm (max norm), and distance should be calculated using both coordinates (ie, don't just calculate distances using only the x or y coordinate)\n",
        "3. Count the number of points in x whose 'x-distance' fall within the distance to the k'th nearest neighbor (calculated above) of $d_j$ (ie, ignore y); call this number $n_{x{_j}}$.  Do the same for the points in y (these are the shaded gray boxes in the image); call it $n_{y{_j}}$.\n",
        "\n",
        "Then, we calculate the final mutual information by averaging over all of the datapoints and adding a few more digamma calls:\n",
        "\n",
        "![final mutual information](https://raw.githubusercontent.com/wingated/cs473/main/labs/images/lab_week_6_image3.png)\n",
        "\n",
        "where $\\Psi$ is the digamma function\n",
        "\n",
        "If you want more details on the implementation, see page 2 of [Estimating Mutual Information](https://arxiv.org/pdf/cond-mat/0305641)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_XyXzpvE9cEx"
      },
      "outputs": [],
      "source": [
        "from scipy.special import digamma\n",
        "\n",
        "\n",
        "def mutual_inf(X, Y, k=3):\n",
        "    # X and Y are lists of length n of samples from p(X,Y)\n",
        "    # this function returns a single scalar\n",
        "\n",
        "    n = len(X)\n",
        "    mi = 0.0\n",
        "\n",
        "    # Compare every row of data to every other row\n",
        "    for j in range(n):\n",
        "        k_smallest = []\n",
        "        x_diffs = []\n",
        "        y_diffs = []\n",
        "\n",
        "        for i in range(n):\n",
        "            if j == i:\n",
        "                continue\n",
        "\n",
        "            # Find the largest distance between X-values and Y-values\n",
        "            x_diffs.append(abs(X[j] - X[i]))\n",
        "            y_diffs.append(abs(Y[j] - Y[i]))\n",
        "\n",
        "            # Find the overall max distance\n",
        "            max_diff = max(x_diffs[-1], y_diffs[-1])\n",
        "\n",
        "            # Save the k smallest max distances between data[j] and all other rows\n",
        "            if len(k_smallest) < k:\n",
        "                k_smallest.append(max_diff)\n",
        "\n",
        "            else:\n",
        "                for index in range(len(k_smallest)):\n",
        "                    if max_diff < k_smallest[index]:\n",
        "                        temp = k_smallest[index]\n",
        "                        k_smallest[index] = max_diff\n",
        "                        max_diff = temp\n",
        "\n",
        "        # at this point, k_smallest contains the k smallest distances for point j\n",
        "        # the k-th nearest neighbor distance is k_smallest[-1]\n",
        "\n",
        "\n",
        "        # Count how many times the largest distance between X-values and Y-values is less than the k-th smallest distance\n",
        "        n_xj = sum(1 for d in x_diffs if d <= k_smallest[-1])\n",
        "        n_yj = sum(1 for d in y_diffs if d <= k_smallest[-1])\n",
        "\n",
        "        mi += digamma(k) - digamma(n_xj + 1) - digamma(n_yj + 1) + digamma(n)\n",
        "\n",
        "    return mi / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAeeGnO9cEx"
      },
      "source": [
        "---\n",
        "## Part 2:\n",
        "\n",
        "Now that you can estimate mutual information between two variables, we're going to use your method in the context of a regression problem.\n",
        "\n",
        "Given our X,y data from the beginning of the lab, run your function and calculate the mutual information between every column of X and y.  This should result in a list of scores, that might look something like this:\n",
        "\n",
        "```python\n",
        "array([1.2641203602210282, 0.07974959311825458, 0.03321433628330617, 0.02213361087954091, 0.0, 0.0, 0.0])\n",
        "```\n",
        "\n",
        "If you wrap this in a single function (call it \"all_mutual_inf\"), then you can use scikit learn's feature selection algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-Yp1DQ5A9cEy"
      },
      "outputs": [],
      "source": [
        "def all_mutual_inf( X, y, k = 3 ):\n",
        "    # return a list of mutual informations\n",
        "    # the list should be as long as the number of columns in X\n",
        "\n",
        "    mi_scores = []\n",
        "    for i in range(X.shape[1]):  # Iterate over each feature (column) in X\n",
        "        mi_score = mutual_inf(X[:, i], y, k)  # Compute MI for the i-th feature\n",
        "        mi_scores.append(mi_score)\n",
        "\n",
        "    # SelectKBest expects two outputs: scores and p-values.\n",
        "    # Since we don't have p-values for this MI estimator,\n",
        "    # return a dummy array of zeros for the p-values.\n",
        "    return mi_scores, np.zeros(len(mi_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq8wwBVk9cEy",
        "outputId": "6a11e932-9cc8-4175-98c0-e97d1e2feed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most informative features: [0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "selector = SelectKBest(all_mutual_inf, k=2)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "top_features = selector.get_support(indices=True)\n",
        "\n",
        "print(\"\\nMost informative features:\", top_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3cFeALY9cEy"
      },
      "source": [
        "So: which X features are most informative about y?\n",
        "\n",
        "Most informative features: [0 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yao5zAC99cEy"
      },
      "source": [
        "---\n",
        "## Hints\n",
        "\n",
        "The following functions may be useful to you: scipy.special.digamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC2MxY-l9cEy"
      },
      "source": [
        "For fun, you could also [read more about the KSG estimator](https://arxiv.org/pdf/1604.03006)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}